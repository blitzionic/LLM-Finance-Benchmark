{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 21:30:48 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import base64\n",
    "import os\n",
    "import re\n",
    "from io import BytesIO\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import openai\n",
    "import requests\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from visual_scoring.score import (\n",
    "    UnifiedQAModel,\n",
    "    VQAModel,\n",
    "    VS_score_single,\n",
    "    filter_question_and_answers,\n",
    "    get_question_and_answers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: shape\n",
      "Device: cuda:0\n",
      "Begin index: 0\n"
     ]
    }
   ],
   "source": [
    "# 传入参数\n",
    "\n",
    "data_type = \"shape\"\n",
    "device = f\"cuda:{0}\"\n",
    "begin_idx = 0\n",
    "print(f\"Data type: {data_type}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Begin index: {begin_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"sk-M5ppriS3vTYSiwFn3c58Af766d7c4956B4EcEc36888a1c2b\"\n",
    "api_base = \"https://ai98.vip/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "os.environ[\"OPENAI_API_BASE\"] = api_base\n",
    "openai.api_key = api_key\n",
    "openai.base_url = api_base\n",
    "client = openai.OpenAI(api_key=api_key, base_url=api_base)\n",
    "\n",
    "def openai_completion(prompt, engine=\"gpt-4o\", max_tokens=700, temperature=0):\n",
    "    resp = client.chat.completions.create(\n",
    "        model=engine,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        stop=[\"\\n\\n\", \"<|endoftext|>\"],\n",
    "    )\n",
    "\n",
    "    return resp.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 21:31:24 | INFO | httpx | HTTP Request: POST https://ai98.vip/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_completion(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "2025-01-07 21:32:51,369 - modelscope - INFO - initiate model from /root/autodl-tmp/models/iic/mplug_visual-question-answering_coco_large_en/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mplug-large...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 21:32:51,372 - modelscope - INFO - initiate model from location /root/autodl-tmp/models/iic/mplug_visual-question-answering_coco_large_en/.\n",
      "2025-01-07 21:32:51,374 - modelscope - INFO - initialize model from /root/autodl-tmp/models/iic/mplug_visual-question-answering_coco_large_en/\n",
      "/root/miniconda3/envs/RPG/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1985: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from /root/autodl-tmp/models/iic/mplug_visual-question-answering_coco_large_en/pytorch_model.bin\n",
      "<All keys matched successfully>\n",
      "Finish loading mplug-large\n",
      "Using SBERT on GPU\n"
     ]
    }
   ],
   "source": [
    "def get_image_from_url(url: str):\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img = img.resize((224, 224))\n",
    "    img = img.convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_image_from_path(file_path: str):\n",
    "    img = Image.open(file_path)\n",
    "    img = img.resize((224, 224))\n",
    "    img = img.convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "\n",
    "def encode_image_from_path(image_path):\n",
    "    \"\"\"\n",
    "    对图片文件进行 Base64 编码\n",
    "\n",
    "    输入：\n",
    "         - image_path：图片的文件路径\n",
    "    输出：\n",
    "         - 编码后的 Base64 字符串\n",
    "    \"\"\"\n",
    "    # 二进制读取模式打开图片文件，\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        # 将编码后的字节串解码为 UTF-8 字符串，以便于在文本环境中使用。\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def encode_image_from_PIL_image(image):\n",
    "    # 创建一个内存缓冲区\n",
    "    buffered = BytesIO()\n",
    "    # 将 PIL 图像对象保存到内存缓冲区中，格式为 JPEG，你也可以选择其他格式\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    # 获取缓冲区中的字节数据并将其编码为 base64 字符串\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return img_str\n",
    "\n",
    "\n",
    "unifiedqa_model = UnifiedQAModel(\"allenai/unifiedqa-v2-t5-large-1363200\", device=device)\n",
    "vqa_model = VQAModel(\"mplug-large\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 21:34:45 | INFO | httpx | HTTP Request: POST https://ai98.vip/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "  'element': 'pitcher of beer',\n",
       "  'question': 'Is there a pitcher of beer?  ',\n",
       "  'choices': ['yes', 'no  '],\n",
       "  'answer': 'yes  ',\n",
       "  'element_type': 'object'},\n",
       " {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "  'element': 'pitcher of beer',\n",
       "  'question': 'What drink is in the pitcher?  ',\n",
       "  'choices': ['beer', 'water', 'juice', 'milk  '],\n",
       "  'answer': 'beer  ',\n",
       "  'element_type': 'object'},\n",
       " {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "  'element': \"elephant's trunk\",\n",
       "  'question': \"Is there an elephant's trunk in the pitcher?  \",\n",
       "  'choices': ['yes', 'no  '],\n",
       "  'answer': 'yes  ',\n",
       "  'element_type': 'object'},\n",
       " {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "  'element': \"elephant's trunk\",\n",
       "  'question': 'What is inside the pitcher along with the beer?  ',\n",
       "  'choices': [\"elephant's trunk\", 'straw', 'stirrer', 'spoon  '],\n",
       "  'answer': \"elephant's trunk  \",\n",
       "  'element_type': 'object'},\n",
       " {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "  'element': 'one',\n",
       "  'question': 'Is there one pitcher of beer?  ',\n",
       "  'choices': ['yes', 'no  '],\n",
       "  'answer': 'yes  ',\n",
       "  'element_type': 'counting'},\n",
       " {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "  'element': 'one',\n",
       "  'question': 'How many pitchers of beer are there?  ',\n",
       "  'choices': ['1', '2', '3', '4  '],\n",
       "  'answer': '1  ',\n",
       "  'element_type': 'counting'},\n",
       " {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "  'element': 'full',\n",
       "  'question': 'Is the pitcher full of beer?  ',\n",
       "  'choices': ['yes', 'no  '],\n",
       "  'answer': 'yes  ',\n",
       "  'element_type': 'attribute'},\n",
       " {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "  'element': 'full',\n",
       "  'question': 'Is the pitcher full or half-full?  ',\n",
       "  'choices': ['full', 'half-full', 'empty', 'almost empty  '],\n",
       "  'answer': 'full  ',\n",
       "  'element_type': 'attribute'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_questions = get_question_and_answers(\"One full pitcher of beer with an elephant's trunk in it.\")\n",
    "gpt_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_VS_result(text, img_path, filtered_questions=None):\n",
    "    if not filtered_questions:\n",
    "        # Generate questions with GPT\n",
    "        gpt_questions = get_question_and_answers(text)\n",
    "\n",
    "        # Filter questions with UnifiedQA\n",
    "        filtered_questions = filter_question_and_answers(unifiedqa_model, gpt_questions)\n",
    "\n",
    "        # See the questions\n",
    "        # print(filtered_questions)\n",
    "\n",
    "        # calucluate VS score\n",
    "        result = VS_score_single(vqa_model, filtered_questions, img_path)\n",
    "        return filtered_questions, result\n",
    "    else:\n",
    "        # calucluate VS score\n",
    "        result = VS_score_single(vqa_model, filtered_questions, img_path)\n",
    "        return result\n",
    "    \n",
    "def generate_image(prompt, model=\"dall-e-3\", size=\"1024x1024\", quality=\"standard\", n=1):\n",
    "    response = client.images.generate(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        size=size,\n",
    "        quality=quality,\n",
    "        n=n,\n",
    "    )\n",
    "\n",
    "    image_url = response.data[0].url\n",
    "    img = get_image_from_url(image_url)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_to_message(\n",
    "    user_prompt, previous_prompts, generated_image, num_solutions, result\n",
    "):\n",
    "    image = encode_image_from_PIL_image(generated_image)\n",
    "\n",
    "    VS_results = []\n",
    "    for i, (key, value) in enumerate(result[\"question_details\"].items()):\n",
    "        VS_result = \"Element \" + str(i) + \"\\n\"\n",
    "        VS_result += \"Question: \" + key + \"\\n\"\n",
    "        VS_result += \"Ground Truth: \" + value[\"answer\"] + \"\\n\"\n",
    "        VS_result += (\n",
    "            \"In the image generated from above prompt, the VQA model identified infer that the answer to the question is: \"\n",
    "            + value[\"free_form_vqa\"]\n",
    "            + \"\\n\"\n",
    "        )\n",
    "\n",
    "        VS_results.append(VS_result)\n",
    "\n",
    "    VS_results = \"\\n\".join(VS_results)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert prompt optimizer for text-to-image models. Text-to-image models take a text prompt as input and generate images depicting the prompt as output. You are responsible for transforming human-written prompts into improved prompts for text-to-image models. Your responses should be concise and effective.\n",
    "\n",
    "Your task is to optimize the human initial prompt: \"{user_prompt}\". Below are some previous prompts along with a breakdown of their visual elements. Each element is paired with a score indicating its presence in the generated image. A score of 1 indicates visual elements matching the human initial prompt, while a score of 0 indicates no match.\n",
    "\n",
    "Here is the image that the text-to-image model generated based on the initial prompt:\n",
    "{{image_placeholder}}\n",
    "\n",
    "Here are the previous prompts and their visual element scores:\n",
    "## Previous Prompts\n",
    "{previous_prompts}\n",
    "## Visual Element Scores\n",
    "{VS_results}\n",
    "\n",
    "Generate {num_solutions} paraphrases of the initial prompt which retain the semantic meaning and have higher scores than all the previous prompts. Prioritize optimizing for objects with the lowest scores. Prefer substitutions and reorderings over additions. Please respond with each new prompt in between <PROMPT> and </PROMPT>, for example:\n",
    "1. <PROMPT>paraphrase 1</PROMPT>\n",
    "2. <PROMPT>paraphrase 2</PROMPT>\n",
    "...\n",
    "{num_solutions}. <PROMPT>paraphrase {num_solutions}</PROMPT>\n",
    "\"\"\"\n",
    "    text_prompts = prompt.split(\"{image_placeholder}\")\n",
    "\n",
    "    user_content = [{\"type\": \"text\", \"text\": text_prompts[0]}]\n",
    "    base64_images = [\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{image}\",\n",
    "                \"detail\": \"high\",\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    user_content.extend(base64_images)\n",
    "    user_content.append({\"type\": \"text\", \"text\": text_prompts[1]})\n",
    "    messages_template = [{\"role\": \"user\", \"content\": user_content}]\n",
    "\n",
    "    return messages_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_chat_response(messages_template, client):\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": messages_template,\n",
    "        \"max_tokens\": 1600,\n",
    "        \"temperature\": 0,\n",
    "        \"seed\": 2024,\n",
    "    }\n",
    "\n",
    "    # 调用 OpenAI API 生成回复\n",
    "    response = client.chat.completions.create(**payload)\n",
    "\n",
    "    # 返回生成的结果\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def extract_prompts(text):\n",
    "    pattern = r\"<PROMPT>(.*?)</PROMPT>\"\n",
    "    prompts = re.findall(pattern, text)\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_retries = 10  # 最大重试次数\n",
    "\n",
    "\n",
    "def DALLE3_VS(prompt):\n",
    "    success = False\n",
    "    retries = 0\n",
    "    print(f\"Generating image for prompt: {prompt}\")\n",
    "    while not success and retries < max_retries:\n",
    "        try:\n",
    "            image = generate_image(prompt=prompt)\n",
    "            success = True\n",
    "            print(\"Image generated successfully!\")\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            print(f\"Error: {e}\")\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying... ({retries}/{max_retries})\")\n",
    "                # time.sleep(1)  # 等待 1 秒后重试\n",
    "            else:\n",
    "                print(\"Max retries reached. Exiting.\")\n",
    "                break\n",
    "    if not success:\n",
    "        print(\"Failed to generate image. Exiting.\")\n",
    "        return\n",
    "\n",
    "    success = False\n",
    "    retries = 0\n",
    "    print(\"Calculating VS score...\")\n",
    "    while not success and retries < max_retries:\n",
    "        try:\n",
    "            filtered_questions, VS_result = get_VS_result(prompt, image)\n",
    "            success = True\n",
    "            print(f\"\\nVS score: {VS_result['VS_score']}\")\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            print(f\"Error: {e}\")\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying... ({retries}/{max_retries})\")\n",
    "                # time.sleep(1)  # 等待 1 秒后重试\n",
    "            else:\n",
    "                print(\"Max retries reached. Exiting.\")\n",
    "                break\n",
    "    if not success:\n",
    "        print(\"Failed to calculate VS score. Exiting.\")\n",
    "        return image\n",
    "\n",
    "    success = False\n",
    "    retries = 0\n",
    "    print(\"Generating new prompt...\")\n",
    "    while not success and retries < max_retries:\n",
    "        try:\n",
    "            formatted_prompt = format_prompt_to_message(\n",
    "                user_prompt=prompt,\n",
    "                previous_prompts=prompt,\n",
    "                generated_image=image,\n",
    "                num_solutions=3,\n",
    "                result=VS_result,\n",
    "            )\n",
    "            generate_prompts = generate_image_chat_response(formatted_prompt, client)\n",
    "            new_regional_prompt = extract_prompts(generate_prompts)[0]\n",
    "            success = True\n",
    "            print(\"Prompt formatted successfully!\")\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            print(f\"Error: {e}\")\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying... ({retries}/{max_retries})\")\n",
    "                # time.sleep(1)  # 等待 1 秒后重试\n",
    "            else:\n",
    "                print(\"Max retries reached. Exiting.\")\n",
    "                break\n",
    "    if not success:\n",
    "        print(\"Failed to generate new prompt. Exiting.\")\n",
    "        return image\n",
    "\n",
    "    print(f\"New prompt generated: {new_regional_prompt}\")\n",
    "    try:\n",
    "        new_image = generate_image(\n",
    "            prompt=prompt,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return image\n",
    "\n",
    "    new_VS_result = get_VS_result(prompt, new_image, filtered_questions)\n",
    "    print(f\"\\nVS score: {new_VS_result['VS_score']}\")\n",
    "\n",
    "    if new_VS_result[\"VS_score\"] > VS_result[\"VS_score\"]:\n",
    "        return new_image\n",
    "    else:\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DALLE3_VS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne full pitcher of beer with an elephant\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms trunk in it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mDALLE3_VS\u001b[49m(prompt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DALLE3_VS' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = \"One full pitcher of beer with an elephant's trunk in it.\"\n",
    "DALLE3_VS(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_retries = 10  # 最大重试次数\n",
    "\n",
    "\n",
    "def generate_image_robust(prompt):\n",
    "    success = False\n",
    "    retries = 0\n",
    "    print(f\"Generating image for prompt: {prompt}\")\n",
    "    while not success and retries < max_retries:\n",
    "        try:\n",
    "            image = generate_image(prompt=prompt)\n",
    "            success = True\n",
    "            print(\"Image generated successfully!\")\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            print(f\"Error: {e}\")\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying... ({retries}/{max_retries})\")\n",
    "                # time.sleep(1)  # 等待 1 秒后重试\n",
    "            else:\n",
    "                print(\"Max retries reached. Exiting.\")\n",
    "                break\n",
    "    if not success:\n",
    "        print(\"Failed to generate image. Exiting.\")\n",
    "        raise Exception(\"Failed to generate image\")\n",
    "    else:\n",
    "        print(\"Image generated successfully!\")\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reviewer:\n",
    "    \"\"\"\n",
    "    Agent A: 审阅者 (Reviewer)\n",
    "    - 负责阅读/审阅初始解或思路，对其正确性、完整性进行评价；\n",
    "    - 主要会找出优点与缺陷，但不一定提出深度质疑或修正方案。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"\\nReviewer initialized.\")\n",
    "        print(\"----------------------\")\n",
    "\n",
    "    def calculate_VS_score(self, prompt, image):\n",
    "        print(\"Calculating VS score...\")\n",
    "        success = False\n",
    "        retries = 0\n",
    "        print(\"Calculating VS score...\")\n",
    "        while not success and retries < max_retries:\n",
    "            try:\n",
    "                filtered_questions, VS_result = get_VS_result(prompt, image)\n",
    "                success = True\n",
    "                print(f\"\\nVS score: {VS_result['VS_score']}\")\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error: {e}\")\n",
    "                if retries < max_retries:\n",
    "                    print(f\"Retrying... ({retries}/{max_retries})\")\n",
    "                    # time.sleep(1)  # 等待 1 秒后重试\n",
    "                else:\n",
    "                    print(\"Max retries reached. Exiting.\")\n",
    "                    break\n",
    "        if not success:\n",
    "            print(\"Failed to calculate VS score. Exiting.\")\n",
    "            raise Exception(\"Failed to calculate VS score.\")\n",
    "        else:\n",
    "            print(\"VS score calculated successfully!\")\n",
    "            return filtered_questions, VS_result\n",
    "        \n",
    "        \n",
    "    def format_prompt_to_message(\n",
    "        self, user_prompt, previous_prompts, generated_image, vs_result\n",
    "    ):\n",
    "        image = encode_image_from_PIL_image(generated_image)\n",
    "\n",
    "        VS_results = []\n",
    "        for i, (key, value) in enumerate(vs_result[\"question_details\"].items()):\n",
    "            VS_result = \"Element \" + str(i) + \"\\n\"\n",
    "            VS_result += \"Question: \" + key + \"\\n\"\n",
    "            VS_result += \"Ground Truth: \" + value[\"answer\"] + \"\\n\"\n",
    "            VS_result += (\n",
    "                \"In the image generated from above prompt, the VQA model identified infer that the answer to the question is: \"\n",
    "                + value[\"free_form_vqa\"]\n",
    "                + \"\\n\"\n",
    "            )\n",
    "\n",
    "            VS_results.append(VS_result)\n",
    "\n",
    "        VS_results = \"\\n\".join(VS_results)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are a prompt reviewer for text-to-image models. Your role is to evaluate both the initial human-written prompt and previous prompts based on their effectiveness in conveying visual elements that match the generated images. Consider the scores assigned to each visual element in the outputs, with 1 indicating a perfect match and 0 indicating no match.\n",
    "\n",
    "Your task is to review the initial prompt: \"{user_prompt}\". Additionally, provide an evaluation of the previous prompts given.\n",
    "\n",
    "Here is the image that the text-to-image model generated based on the initial prompt:\n",
    "{{image_placeholder}}\n",
    "\n",
    "Here are the previous prompts and their visual element scores:\n",
    "## Previous Prompts\n",
    "{previous_prompts}\n",
    "## Visual Element Scores\n",
    "{VS_results}\n",
    "\n",
    "Provide a comprehensive evaluation of the initial prompt and each of the previous prompts. Focus on the correctness and completeness of each prompt in relation to the generated images, highlighting strengths and weaknesses. Depth questioning or suggested alterations are not necessary, but insightful commentary is encouraged.\n",
    "If there are no previous prompts, simply provide an evaluation for the initial prompt. Respond with each evaluation in between <EVALUATION> and </EVALUATION> as follows:\n",
    "\n",
    "1. <EVALUATION>Your Evaluation for initial prompt</EVALUATION>\n",
    "2. <EVALUATION>Your Evaluation for previous prompt 1</EVALUATION>\n",
    "...\n",
    "n. <EVALUATION>Your Evaluation for previous prompt n</EVALUATION>\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        text_prompts = prompt.split(\"{image_placeholder}\")\n",
    "\n",
    "        user_content = [{\"type\": \"text\", \"text\": text_prompts[0]}]\n",
    "        base64_images = [\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{image}\",\n",
    "                    \"detail\": \"high\",\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "        user_content.extend(base64_images)\n",
    "        user_content.append({\"type\": \"text\", \"text\": text_prompts[1]})\n",
    "        messages_template = [{\"role\": \"user\", \"content\": user_content}]\n",
    "\n",
    "        return messages_template\n",
    "        \n",
    "    def generate_response(self, user_prompt, generated_image, previous_prompts=None):\n",
    "        filtered_questions, VS_result = self.calculate_VS_score(user_prompt, generated_image)\n",
    "        formatted_prompt = self.format_prompt_to_message(\n",
    "                user_prompt=user_prompt,\n",
    "                generated_image=generated_image,\n",
    "                previous_prompts=previous_prompts,\n",
    "                vs_result=VS_result,\n",
    "            )\n",
    "        print(\"Generating evaluation response...\")\n",
    "        response = generate_image_chat_response(formatted_prompt, client)\n",
    "        return filtered_questions, VS_result, response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reviewer initialized.\n",
      "----------------------\n",
      "Generating image for prompt: One full pitcher of beer with an elephant's trunk in it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 21:37:33 | INFO | httpx | HTTP Request: POST https://ai98.vip/v1/images/generations \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image generated successfully!\n",
      "Image generated successfully!\n",
      "Calculating VS score...\n",
      "Calculating VS score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 21:38:20 | INFO | httpx | HTTP Request: POST https://ai98.vip/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: mean requires at least one data point\n",
      "Retrying... (1/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 21:38:26 | INFO | httpx | HTTP Request: POST https://ai98.vip/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: mean requires at least one data point\n",
      "Retrying... (2/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 21:38:32 | INFO | httpx | HTTP Request: POST https://ai98.vip/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: mean requires at least one data point\n",
      "Retrying... (3/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 21:38:40 | INFO | httpx | HTTP Request: POST https://ai98.vip/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]/root/miniconda3/envs/RPG/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/RPG/lib/python3.9/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 8/8 [00:03<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VS score: 0.625\n",
      "VS score calculated successfully!\n",
      "Generating evaluation response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 21:38:50 | INFO | httpx | HTTP Request: POST https://ai98.vip/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<EVALUATION>The initial prompt “One full pitcher of beer with an elephant's trunk in it.” has a mixed effectiveness in conveying visual elements to match the generated image. \\n\\nStrengths:\\n- The image clearly shows a beer and an elephant's trunk, which aligns with the elements described in the prompt (Element 0 and Element 4 both scored well).\\n- The beverage is correctly identified as beer (Element 3).\\n\\nWeaknesses:\\n- The prompt specifies a “pitcher” of beer, but the image shows a mug, which does not match the description (Element 1). \\n- Additionally, while the prompt indicates that the pitcher is full, the model identified it as not full (Element 6), which is inconsistent.\\n\\nOverall, the prompt conveys key elements effectively, but the type of container and its fullness were mismatched in the generated image. The visual depiction of the elephant’s trunk is accurate according to the prompt.</EVALUATION>\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"One full pitcher of beer with an elephant's trunk in it.\"\n",
    "reviewer = Reviewer()\n",
    "image = generate_image_robust(prompt)\n",
    "filtered_questions, VS_result, reviewer_evaluation = reviewer.generate_response(prompt, image)\n",
    "reviewer_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "   'element': 'pitcher',\n",
       "   'question': 'is there a pitcher of beer?',\n",
       "   'choices': ['yes', 'no'],\n",
       "   'answer': 'yes',\n",
       "   'element_type': 'object',\n",
       "   'free_form_vqa': 'yes',\n",
       "   'multiple_choice_vqa': 'yes',\n",
       "   'scores': 1},\n",
       "  {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "   'element': 'pitcher',\n",
       "   'question': 'what type of container is holding the beer?',\n",
       "   'choices': ['pitcher', 'bottle', 'glass', 'cup'],\n",
       "   'answer': 'pitcher',\n",
       "   'element_type': 'object',\n",
       "   'free_form_vqa': 'mug',\n",
       "   'multiple_choice_vqa': 'cup',\n",
       "   'scores': 0},\n",
       "  {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "   'element': 'beer',\n",
       "   'question': 'is the pitcher filled with beer?',\n",
       "   'choices': ['yes', 'no'],\n",
       "   'answer': 'yes',\n",
       "   'element_type': 'food',\n",
       "   'free_form_vqa': 'yes',\n",
       "   'multiple_choice_vqa': 'yes',\n",
       "   'scores': 1},\n",
       "  {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "   'element': 'beer',\n",
       "   'question': 'what beverage is in the pitcher?',\n",
       "   'choices': ['beer', 'water', 'juice', 'soda'],\n",
       "   'answer': 'beer',\n",
       "   'element_type': 'food',\n",
       "   'free_form_vqa': 'beer',\n",
       "   'multiple_choice_vqa': 'beer',\n",
       "   'scores': 1},\n",
       "  {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "   'element': \"elephant's trunk\",\n",
       "   'question': \"is an elephant's trunk in the pitcher?\",\n",
       "   'choices': ['yes', 'no'],\n",
       "   'answer': 'yes',\n",
       "   'element_type': 'animal/human',\n",
       "   'free_form_vqa': 'yes',\n",
       "   'multiple_choice_vqa': 'yes',\n",
       "   'scores': 1},\n",
       "  {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "   'element': 'one',\n",
       "   'question': 'is there one pitcher of beer?',\n",
       "   'choices': ['yes', 'no'],\n",
       "   'answer': 'yes',\n",
       "   'element_type': 'counting',\n",
       "   'free_form_vqa': 'no',\n",
       "   'multiple_choice_vqa': 'no',\n",
       "   'scores': 0},\n",
       "  {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "   'element': 'full',\n",
       "   'question': 'is the pitcher full?',\n",
       "   'choices': ['yes', 'no'],\n",
       "   'answer': 'yes',\n",
       "   'element_type': 'attribute',\n",
       "   'free_form_vqa': 'no',\n",
       "   'multiple_choice_vqa': 'no',\n",
       "   'scores': 0},\n",
       "  {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "   'element': 'full',\n",
       "   'question': 'is the pitcher full or half-empty?',\n",
       "   'choices': ['full', 'half-empty', 'empty', 'overflowing'],\n",
       "   'answer': 'full',\n",
       "   'element_type': 'attribute',\n",
       "   'free_form_vqa': 'full',\n",
       "   'multiple_choice_vqa': 'full',\n",
       "   'scores': 1}],\n",
       " {'VS_score': 0.625,\n",
       "  'question_details': {'is there a pitcher of beer?': {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "    'element': 'pitcher',\n",
       "    'question': 'is there a pitcher of beer?',\n",
       "    'choices': ['yes', 'no'],\n",
       "    'answer': 'yes',\n",
       "    'element_type': 'object',\n",
       "    'free_form_vqa': 'yes',\n",
       "    'multiple_choice_vqa': 'yes',\n",
       "    'scores': 1},\n",
       "   'what type of container is holding the beer?': {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "    'element': 'pitcher',\n",
       "    'question': 'what type of container is holding the beer?',\n",
       "    'choices': ['pitcher', 'bottle', 'glass', 'cup'],\n",
       "    'answer': 'pitcher',\n",
       "    'element_type': 'object',\n",
       "    'free_form_vqa': 'mug',\n",
       "    'multiple_choice_vqa': 'cup',\n",
       "    'scores': 0},\n",
       "   'is the pitcher filled with beer?': {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "    'element': 'beer',\n",
       "    'question': 'is the pitcher filled with beer?',\n",
       "    'choices': ['yes', 'no'],\n",
       "    'answer': 'yes',\n",
       "    'element_type': 'food',\n",
       "    'free_form_vqa': 'yes',\n",
       "    'multiple_choice_vqa': 'yes',\n",
       "    'scores': 1},\n",
       "   'what beverage is in the pitcher?': {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "    'element': 'beer',\n",
       "    'question': 'what beverage is in the pitcher?',\n",
       "    'choices': ['beer', 'water', 'juice', 'soda'],\n",
       "    'answer': 'beer',\n",
       "    'element_type': 'food',\n",
       "    'free_form_vqa': 'beer',\n",
       "    'multiple_choice_vqa': 'beer',\n",
       "    'scores': 1},\n",
       "   \"is an elephant's trunk in the pitcher?\": {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "    'element': \"elephant's trunk\",\n",
       "    'question': \"is an elephant's trunk in the pitcher?\",\n",
       "    'choices': ['yes', 'no'],\n",
       "    'answer': 'yes',\n",
       "    'element_type': 'animal/human',\n",
       "    'free_form_vqa': 'yes',\n",
       "    'multiple_choice_vqa': 'yes',\n",
       "    'scores': 1},\n",
       "   'is there one pitcher of beer?': {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "    'element': 'one',\n",
       "    'question': 'is there one pitcher of beer?',\n",
       "    'choices': ['yes', 'no'],\n",
       "    'answer': 'yes',\n",
       "    'element_type': 'counting',\n",
       "    'free_form_vqa': 'no',\n",
       "    'multiple_choice_vqa': 'no',\n",
       "    'scores': 0},\n",
       "   'is the pitcher full?': {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "    'element': 'full',\n",
       "    'question': 'is the pitcher full?',\n",
       "    'choices': ['yes', 'no'],\n",
       "    'answer': 'yes',\n",
       "    'element_type': 'attribute',\n",
       "    'free_form_vqa': 'no',\n",
       "    'multiple_choice_vqa': 'no',\n",
       "    'scores': 0},\n",
       "   'is the pitcher full or half-empty?': {'caption': \"One full pitcher of beer with an elephant's trunk in it.\",\n",
       "    'element': 'full',\n",
       "    'question': 'is the pitcher full or half-empty?',\n",
       "    'choices': ['full', 'half-empty', 'empty', 'overflowing'],\n",
       "    'answer': 'full',\n",
       "    'element_type': 'attribute',\n",
       "    'free_form_vqa': 'full',\n",
       "    'multiple_choice_vqa': 'full',\n",
       "    'scores': 1}}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_questions, VS_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. <EVALUATION>The initial prompt \"The brown dog is lying on the blue pillow.\" effectively conveys visual elements that closely match the generated image. The visual element scores indicate that the key aspects of the prompt were successfully captured: there is a dog, it is brown, there is a pillow, and the pillow is blue. Most importantly, the brown dog is on the pillow, which aligns with the prompt's description, although there is a minor discrepancy in element 9 where it is noted as \"next to it\" instead of \"on.\" Overall, the initial prompt is highly accurate and comprehensive in representing the elements present in the generated image.</EVALUATION>\n"
     ]
    }
   ],
   "source": [
    "print(reviewer_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Challenger:\n",
    "    \"\"\"\n",
    "    Agent B: 质疑者 (Challenger)\n",
    "    - 负责对已给出的解进行“质疑”或“攻击”，找出潜在漏洞、不满足约束之处；\n",
    "    - 可能提出改进思路，或抛出新的反例/约束来检验当前解。\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"\\nChallenger initialized.\")\n",
    "        print(\"----------------------\")\n",
    "        \n",
    "    def format_prompt_to_message(\n",
    "        self, user_prompt, previous_prompts, generated_image, vs_result, reviewer_evaluation\n",
    "    ):\n",
    "        image = encode_image_from_PIL_image(generated_image)\n",
    "\n",
    "        VS_results = []\n",
    "        for i, (key, value) in enumerate(vs_result[\"question_details\"].items()):\n",
    "            VS_result = \"Element \" + str(i) + \"\\n\"\n",
    "            VS_result += \"Question: \" + key + \"\\n\"\n",
    "            VS_result += \"Ground Truth: \" + value[\"answer\"] + \"\\n\"\n",
    "            VS_result += (\n",
    "                \"In the image generated from above prompt, the VQA model identified infer that the answer to the question is: \"\n",
    "                + value[\"free_form_vqa\"]\n",
    "                + \"\\n\"\n",
    "            )\n",
    "\n",
    "            VS_results.append(VS_result)\n",
    "\n",
    "        VS_results = \"\\n\".join(VS_results)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are a prompt challenger for text-to-image models. Your role is to critically evaluate the initial human-written prompt and previous prompts, identifying potential flaws and constraints that are not met based on the evaluation of the reviewer. Consider the scores assigned to each visual element in the outputs, with 1 indicating a perfect match and 0 indicating no match.\n",
    "\n",
    "Your task is to challenge the initial prompt: \"{user_prompt}\". Additionally, provide a critique of the previous prompts given.\n",
    "\n",
    "Here is the image that the text-to-image model generated based on the initial prompt:\n",
    "{{image_placeholder}}\n",
    "\n",
    "Here are the previous prompts and their visual element scores:\n",
    "## Previous Prompts\n",
    "{previous_prompts}\n",
    "## Visual Element Scores\n",
    "{VS_results}\n",
    "## Reviewer's Evaluation\n",
    "{reviewer_evaluation}\n",
    "\n",
    "Based on the correctness and completeness of each prompt in relation to the generated images, identify potential weaknesses and unmet constraints. Propose improvement ideas or introduce new counterexamples and constraints to test the current solutions.\n",
    "If there are no previous prompts, focus on challenging the initial prompt. Respond with each challenge in between <CHALLENGE> and </CHALLENGE> as follows:\n",
    "\n",
    "1. <CHALLENGE>Your Challenge for initial prompt</CHALLENGE>\n",
    "2. <CHALLENGE>Your Challenge for previous prompt 1</CHALLENGE>\n",
    "...\n",
    "n. <CHALLENGE>Your Challenge for previous prompt n</CHALLENGE>\n",
    "\n",
    "\"\"\"\n",
    "        # print(prompt)\n",
    "        text_prompts = prompt.split(\"{image_placeholder}\")\n",
    "\n",
    "        user_content = [{\"type\": \"text\", \"text\": text_prompts[0]}]\n",
    "        base64_images = [\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{image}\",\n",
    "                    \"detail\": \"high\",\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "        user_content.extend(base64_images)\n",
    "        user_content.append({\"type\": \"text\", \"text\": text_prompts[1]})\n",
    "        messages_template = [{\"role\": \"user\", \"content\": user_content}]\n",
    "\n",
    "        return messages_template\n",
    "    \n",
    "    def generate_response(self, user_prompt, generated_image, filtered_questions, VS_result, reviewer_evaluation, previous_prompts=None):\n",
    "        formatted_prompt = self.format_prompt_to_message(\n",
    "                user_prompt=user_prompt,\n",
    "                generated_image=generated_image,\n",
    "                previous_prompts=previous_prompts,\n",
    "                vs_result=VS_result,\n",
    "                reviewer_evaluation=reviewer_evaluation\n",
    "            )\n",
    "            \n",
    "        print(\"Generating challenge response...\")\n",
    "        response = generate_image_chat_response(formatted_prompt, client)\n",
    "        return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Challenger initialized.\n",
      "----------------------\n",
      "Generating challenge response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 10:46:14 | INFO | httpx | HTTP Request: POST https://ai98.vip/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "challenger = Challenger()\n",
    "challenger_response = challenger.generate_response(prompt, image, filtered_questions, VS_result, reviewer_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. <CHALLENGE>The initial prompt \"The brown dog is lying on the blue pillow.\" generally aligns well with the generated image. However, there is a minor discrepancy in Element 9, where it is noted that the dog is \"next to it\" rather than explicitly \"on\" the pillow. To improve clarity and precision, the prompt could specify the dog\\'s position more explicitly to avoid ambiguity, such as \"The brown dog is lying directly atop the blue pillow, resting fully on it.\"</CHALLENGE>'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "challenger_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Refiner:\n",
    "    '''\n",
    "    Agent C: 修正者 (Refiner / Fixer)\n",
    "    - 收到来自审阅者、质疑者的反馈后，对当前解进行修改、修补、重构；\n",
    "    - 目标是提高解的质量，使之更符合目标需求或约束。\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"\\nRefiner initialized.\")\n",
    "        print(\"----------------------\")\n",
    "\n",
    "    def format_prompt_to_message(\n",
    "        self, user_prompt, previous_prompts, generated_image, vs_result, reviewer_evaluation, challenger_response\n",
    "    ):\n",
    "        image = encode_image_from_PIL_image(generated_image)\n",
    "\n",
    "        VS_results = []\n",
    "        for i, (key, value) in enumerate(vs_result[\"question_details\"].items()):\n",
    "            VS_result = \"Element \" + str(i) + \"\\n\"\n",
    "            VS_result += \"Question: \" + key + \"\\n\"\n",
    "            VS_result += \"Ground Truth: \" + value[\"answer\"] + \"\\n\"\n",
    "            VS_result += (\n",
    "                \"In the image generated from above prompt, the VQA model identified infer that the answer to the question is: \"\n",
    "                + value[\"free_form_vqa\"]\n",
    "                + \"\\n\"\n",
    "            )\n",
    "\n",
    "            VS_results.append(VS_result)\n",
    "\n",
    "        VS_results = \"\\n\".join(VS_results)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are a prompt refiner for text-to-image models. Your role is to improve the quality of the initial human-written prompt and previous prompts by incorporating feedback received from the reviewer and challenger. Your goal is to adjust, refine, and reconstruct the prompts to better meet the intended requirements and constraints.\n",
    "\n",
    "Your task is to refine the initial prompt: \"{user_prompt}\" and the previous prompts based on the feedback received.\n",
    "\n",
    "Here is the image that the text-to-image model generated based on the initial prompt:\n",
    "{{image_placeholder}}\n",
    "\n",
    "Here are the previous prompts and their visual element scores:\n",
    "## Previous Prompts\n",
    "{previous_prompts}\n",
    "## Visual Element Scores\n",
    "{VS_results}\n",
    "## Reviewer's Evaluation\n",
    "{reviewer_evaluation}\n",
    "## Challenger's Challenge\n",
    "{challenger_response}\n",
    "\n",
    "Using the feedback from both the reviewer and the challenger, modify and enhance the prompts to address weaknesses and fulfill unmet constraints. Generate improved prompts that capture the intended visual elements more effectively.\n",
    "If there are no previous prompts, focus on refining the initial prompt. Respond with each refined prompt in between <REFINED_PROMPT> and </REFINED_PROMPT> as follows:\n",
    "\n",
    "<REFINED_PROMPT>Your Refined prompt</REFINED_PROMPT>\n",
    "\"\"\"\n",
    "        # print(prompt)\n",
    "        text_prompts = prompt.split(\"{image_placeholder}\")\n",
    "\n",
    "        user_content = [{\"type\": \"text\", \"text\": text_prompts[0]}]\n",
    "        base64_images = [\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{image}\",\n",
    "                    \"detail\": \"high\",\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "        user_content.extend(base64_images)\n",
    "        user_content.append({\"type\": \"text\", \"text\": text_prompts[1]})\n",
    "        messages_template = [{\"role\": \"user\", \"content\": user_content}]\n",
    "\n",
    "        return messages_template\n",
    "    \n",
    "    def generate_response(self, user_prompt, generated_image, filtered_questions, VS_result, reviewer_evaluation, challenger_response, previous_prompts=None):\n",
    "        formatted_prompt = self.format_prompt_to_message(\n",
    "                user_prompt=user_prompt,\n",
    "                generated_image=generated_image,\n",
    "                previous_prompts=previous_prompts,\n",
    "                vs_result=VS_result,\n",
    "                reviewer_evaluation=reviewer_evaluation,\n",
    "                challenger_response=challenger_response\n",
    "            )\n",
    "            \n",
    "        print(\"Generating refiner response...\")\n",
    "        response = generate_image_chat_response(formatted_prompt, client)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Refiner initialized.\n",
      "----------------------\n",
      "Generating refiner response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 10:46:22 | INFO | httpx | HTTP Request: POST https://ai98.vip/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "refiner = Refiner()\n",
    "refiner_response = refiner.generate_response(prompt, image, filtered_questions, VS_result, reviewer_evaluation, challenger_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The brown dog is lying directly atop the blue pillow, resting fully on it.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_refine_prompts(text):\n",
    "    pattern = r\"<REFINED_PROMPT>(.*?)</REFINED_PROMPT>\"\n",
    "    prompts = re.findall(pattern, text)\n",
    "    return prompts[0]\n",
    "\n",
    "refine_prompts = extract_refine_prompts(refiner_response)\n",
    "refine_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating image for prompt: The brown dog is lying directly atop the blue pillow, resting fully on it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 10:46:38 | INFO | httpx | HTTP Request: POST https://ai98.vip/v1/images/generations \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image generated successfully!\n",
      "Image generated successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 12.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'VS_score': 0.9,\n",
       " 'question_details': {'Is there a dog?': {'caption': 'The brown dog is lying on the blue pillow.',\n",
       "   'element': 'dog',\n",
       "   'question': 'Is there a dog?',\n",
       "   'choices': ['yes', 'no'],\n",
       "   'answer': 'yes',\n",
       "   'element_type': 'animal/human',\n",
       "   'free_form_vqa': 'yes',\n",
       "   'multiple_choice_vqa': 'yes',\n",
       "   'scores': 1},\n",
       "  'What animal is featured in the description?': {'caption': 'The brown dog is lying on the blue pillow.',\n",
       "   'element': 'dog',\n",
       "   'question': 'What animal is featured in the description?',\n",
       "   'choices': ['dog', 'cat', 'bird', 'fish'],\n",
       "   'answer': 'dog',\n",
       "   'element_type': 'animal/human',\n",
       "   'free_form_vqa': 'dog',\n",
       "   'multiple_choice_vqa': 'dog',\n",
       "   'scores': 1},\n",
       "  'Is there a pillow?': {'caption': 'The brown dog is lying on the blue pillow.',\n",
       "   'element': 'pillow',\n",
       "   'question': 'Is there a pillow?',\n",
       "   'choices': ['yes', 'no'],\n",
       "   'answer': 'yes',\n",
       "   'element_type': 'object',\n",
       "   'free_form_vqa': 'yes',\n",
       "   'multiple_choice_vqa': 'yes',\n",
       "   'scores': 1},\n",
       "  'Is the brown dog lying on something?': {'caption': 'The brown dog is lying on the blue pillow.',\n",
       "   'element': 'lying',\n",
       "   'question': 'Is the brown dog lying on something?',\n",
       "   'choices': ['yes', 'no'],\n",
       "   'answer': 'yes',\n",
       "   'element_type': 'activity',\n",
       "   'free_form_vqa': 'yes',\n",
       "   'multiple_choice_vqa': 'yes',\n",
       "   'scores': 1},\n",
       "  'Is the dog brown?': {'caption': 'The brown dog is lying on the blue pillow.',\n",
       "   'element': 'brown',\n",
       "   'question': 'Is the dog brown?',\n",
       "   'choices': ['yes', 'no'],\n",
       "   'answer': 'yes',\n",
       "   'element_type': 'color',\n",
       "   'free_form_vqa': 'yes',\n",
       "   'multiple_choice_vqa': 'yes',\n",
       "   'scores': 1},\n",
       "  'What color is the dog?': {'caption': 'The brown dog is lying on the blue pillow.',\n",
       "   'element': 'brown',\n",
       "   'question': 'What color is the dog?',\n",
       "   'choices': ['brown', 'black', 'white', 'gray'],\n",
       "   'answer': 'brown',\n",
       "   'element_type': 'color',\n",
       "   'free_form_vqa': 'brown',\n",
       "   'multiple_choice_vqa': 'brown',\n",
       "   'scores': 1},\n",
       "  'Is the pillow blue?': {'caption': 'The brown dog is lying on the blue pillow.',\n",
       "   'element': 'blue',\n",
       "   'question': 'Is the pillow blue?',\n",
       "   'choices': ['yes', 'no'],\n",
       "   'answer': 'yes',\n",
       "   'element_type': 'color',\n",
       "   'free_form_vqa': 'yes',\n",
       "   'multiple_choice_vqa': 'yes',\n",
       "   'scores': 1},\n",
       "  'What color is the pillow?': {'caption': 'The brown dog is lying on the blue pillow.',\n",
       "   'element': 'blue',\n",
       "   'question': 'What color is the pillow?',\n",
       "   'choices': ['blue', 'red', 'yellow', 'green'],\n",
       "   'answer': 'blue',\n",
       "   'element_type': 'color',\n",
       "   'free_form_vqa': 'blue',\n",
       "   'multiple_choice_vqa': 'blue',\n",
       "   'scores': 1},\n",
       "  'Is the dog on the pillow?': {'caption': 'The brown dog is lying on the blue pillow.',\n",
       "   'element': 'on',\n",
       "   'question': 'Is the dog on the pillow?',\n",
       "   'choices': ['yes', 'no'],\n",
       "   'answer': 'yes',\n",
       "   'element_type': 'spatial',\n",
       "   'free_form_vqa': 'yes',\n",
       "   'multiple_choice_vqa': 'yes',\n",
       "   'scores': 1},\n",
       "  'Is the dog on or next to the pillow?': {'caption': 'The brown dog is lying on the blue pillow.',\n",
       "   'element': 'on',\n",
       "   'question': 'Is the dog on or next to the pillow?',\n",
       "   'choices': ['on', 'next to', 'under', 'behind'],\n",
       "   'answer': 'on',\n",
       "   'element_type': 'spatial',\n",
       "   'free_form_vqa': 'next to pillow',\n",
       "   'multiple_choice_vqa': 'next to',\n",
       "   'scores': 0}}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_image = generate_image_robust(refine_prompts)\n",
    "\n",
    "new_VS_result = get_VS_result(prompt, new_image, filtered_questions)\n",
    "\n",
    "new_VS_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_image(image, new_image, VS_result, new_VS_result):\n",
    "    if new_VS_result[\"VS_score\"] > VS_result[\"VS_score\"]:\n",
    "        return new_image\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "best_image = choose_best_image(image, new_image, VS_result, new_VS_result).save(\"best_image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RPG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
