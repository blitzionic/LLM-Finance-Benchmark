{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import faiss\n",
    "# import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_path = r\"..\\..\\agents\"\n",
    "if agents_path not in sys.path:\n",
    "    sys.path.append(agents_path)\n",
    "utils_path = r\"..\\..\\utils\"\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_agent import Agent\n",
    "from initial_generator import InitialGeneratorAgent\n",
    "from reviewer import ReviewerAgent\n",
    "from challenger import ChallengerAgent\n",
    "from refiner import RefinerAgent\n",
    "# from decider import DeciderAgent\n",
    "\n",
    "from utils import load_dataset, save_results, calculate_score, load_config, get_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\") \n",
    "print(openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasheets_directory = r\"..\\..\\data\\question_sheets\"\n",
    "datasheets_files = glob.glob(os.path.join(datasheets_directory, \"*.csv\"))\n",
    "# print(f\"Found {len(datasheets_files)} CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV files...:   0%|          | 0/82 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ..\\..\\data\\question_sheets\\Asset.csv\n",
      "TOPIC: asset\n",
      "ROLE DESCRIPTION: You are an asset management expert, skilled in evaluating and managing various financial assets.\n",
      "Dataset loaded successfully from ..\\..\\data\\question_sheets\\Asset.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On December 31, Strike Company sold one of its batting cages for $55,000. The equipment had an initial cost of $310,000 and has accumulated depreciation of $260,000. Depreciation has been taken up to the end of the year. What is the amount of the gain or loss on this transaction?\n",
      "\n",
      "\n",
      "A: $10,000 loss\n",
      "B: $5,000 gain\n",
      "C: $5,000 loss\n",
      "D: $10,000 gain\n",
      "\n",
      "SYSTEM PROMPT: Provide an answer to the following finance question(s).Answer the following multiple-choice question by selecting one letter: A, B, C, or D.\n",
      "initial response: {'answer': 'B', 'reasoning': 'To determine the gain or loss on the sale of the batting cage, we need to calculate the book value of the equipment at the time of sale and compare it to the sale price. \\n\\nThe book value is calculated as the initial cost minus accumulated depreciation:\\n\\nBook Value = Initial Cost - Accumulated Depreciation\\nBook Value = $310,000 - $260,000 = $50,000\\n\\nThe equipment was sold for $55,000. The gain or loss is the difference between the sale price and the book value:\\n\\nGain/Loss = Sale Price - Book Value\\nGain/Loss = $55,000 - $50,000 = $5,000\\n\\nSince the sale price is higher than the book value, there is a gain of $5,000. Therefore, the correct answer is B: $5,000 gain.'}\n",
      "SYSTEM PROMPT: You are an asset management expert, skilled in evaluating and managing various financial assets.\n",
      "You are a highly experienced financial expert. Your job is to review an answer generated by an initial solution generator for a multiple-choice finance question. Consider the original question, the provided initial answer, and your own expertise in finance to critically assess the answerâ€™s strengths and weaknesses. Identify any gaps in reasoning or potential errors, and then determine what you believe is the correct answer. Finally, respond by selecting one letter (A, B, C, or D) as your recommended answer and provide a clear, concise explanation for your decision. Your response must be formatted as a JSON object with two keys: 'answer' and 'feedback'.\n"
     ]
    }
   ],
   "source": [
    "'''''\n",
    "1. Initial Generation\n",
    "'''''\n",
    "\n",
    "initialAgent = InitialGeneratorAgent(model=\"gpt-4o\")\n",
    "\n",
    "initial_generator_agent_results = [] \n",
    "\n",
    "config_directory = os.path.join(\"..\\..\\config\")\n",
    "topic_roles_path = os.path.join(config_directory, \"topic_roles.json\")\n",
    "results_directory = os.path.join(\"..\\..\\data\", \"results\")\n",
    "os.makedirs(results_directory, exist_ok=True)\n",
    "\n",
    "for sheet in tqdm(datasheets_files, desc = \"Processing CSV files...\"):\n",
    "  print(f\"Processing file: {sheet}\")\n",
    "\n",
    "  base_name = os.path.basename(sheet)           \n",
    "  sheet_name, ext = os.path.splitext(base_name) \n",
    "\n",
    "  ReviewerAgent = ReviewerAgent(topic = sheet_name, model = \"gpt-4o\", topic_roles_json=topic_roles_path)\n",
    "\n",
    "  dataset = load_dataset(sheet)\n",
    "  \n",
    "  initial_guesses = []\n",
    "  reviewer_guesses = [] \n",
    "  for index, row in tqdm(dataset.iterrows(), total=len(dataset), desc=\"Processing rows...\", leave=False):\n",
    "    question = (\n",
    "      f\"{row['question']}\\n\"\n",
    "      f\"A: {row['A']}\\n\"\n",
    "      f\"B: {row['B']}\\n\"\n",
    "      f\"C: {row['C']}\\n\"\n",
    "      f\"D: {row['D']}\\n\"\n",
    "    )\n",
    "\n",
    "    print(question)\n",
    "\n",
    "    initial_response = initialAgent.process(question)\n",
    "    print(f\"initial response: {initial_response}\")\n",
    "    initial_guesses.append(initial_response)\n",
    "    # combines the 'answer' and 'reasoning' in initial response dictionary\n",
    "    combined_initial_guess = f\"Answer: {initial_response.get('answer')}\\nReasoning: {initial_response.get('reasoning')}\"\n",
    "    reviewer_response = ReviewerAgent.process(question, combined_initial_guess)\n",
    "    reviewer_guesses.append(reviewer_response)\n",
    "    print(f\"Reveiewer response: {reviewer_guesses}\")\n",
    "\n",
    "  dataset[\"initial_guess\"] = initial_guesses\n",
    "  dataset[\"reveiwer_quess\"] = reviewer_guesses\n",
    "\n",
    "  # check correctness \n",
    "  initial_accuracy = calculate_score(dataset, answer_column=\"answer\", guess_column=\"inital_guess\")\n",
    "  print(f\"Accuracy for {os.path.basename(sheet)}: {initial_accuracy:.2f}%\")\n",
    "  \n",
    "  reveiwer_accuracy = calculate_score(dataset, answer_column=\"answer\", guess_column=\"reviewer_guess\")\n",
    "  print(f\"Accuracy for {os.path.basename(sheet)}: {initial_accuracy:.2f}%\")\n",
    "\n",
    "  output_filename = f\"{sheet_name}_gpt-4o{ext}\"\n",
    "\n",
    "  output_file = os.path.join(results_directory, output_filename)\n",
    "  save_results(dataset, output_file)\n",
    "  \n",
    "  initial_generator_agent_results.append(dataset)\n",
    "  \n",
    "  break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
